{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd72fac",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from game import GameState\n",
    "from AI.network import QuoridorNet, create_network\n",
    "from AI.alpha_mcts import AlphaMCTS\n",
    "from AI.encoder import encode_state\n",
    "from AI.action_utils import action_to_index, get_legal_action_mask, index_to_action\n",
    "\n",
    "from constants import (\n",
    "    SELF_PLAY_GAMES, REPLAY_BUFFER_SIZE, BATCH_SIZE,\n",
    "    LEARNING_RATE, WEIGHT_DECAY, TRAINING_ITERATIONS,\n",
    "    CHECKPOINT_INTERVAL, TEMP_THRESHOLD, TEMP_INIT, TEMP_FINAL,\n",
    "    ALPHA_MCTS_SIMULATIONS, ACTION_SPACE_SIZE\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3c0db",
   "metadata": {},
   "source": [
    "## 2. Training Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b10eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingExample:\n",
    "    \"\"\"A single training example from self-play.\"\"\"\n",
    "    state: np.ndarray          # Encoded board state\n",
    "    policy: np.ndarray         # MCTS visit count distribution\n",
    "    value: float               # Game outcome from this player's perspective\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-size buffer to store training examples from self-play.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = REPLAY_BUFFER_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, example: TrainingExample) -> None:\n",
    "        \"\"\"Add a training example to the buffer.\"\"\"\n",
    "        self.buffer.append(example)\n",
    "    \n",
    "    def add_batch(self, examples: list[TrainingExample]) -> None:\n",
    "        \"\"\"Add multiple training examples.\"\"\"\n",
    "        for ex in examples:\n",
    "            self.buffer.append(ex)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> list[TrainingExample]:\n",
    "        \"\"\"Sample a random batch of examples.\"\"\"\n",
    "        return random.sample(list(self.buffer), min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all examples.\"\"\"\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset wrapper for training examples.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples: list[TrainingExample]):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        ex = self.examples[idx]\n",
    "        state = torch.FloatTensor(ex.state)\n",
    "        policy = torch.FloatTensor(ex.policy)\n",
    "        value = torch.FloatTensor([ex.value])\n",
    "        return state, policy, value\n",
    "\n",
    "\n",
    "print(\"Data structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9884bfa5",
   "metadata": {},
   "source": [
    "## 3. Self-Play Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f62136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_game(\n",
    "    network: QuoridorNet,\n",
    "    num_simulations: int = ALPHA_MCTS_SIMULATIONS,\n",
    "    temp_threshold: int = TEMP_THRESHOLD,\n",
    "    verbose: bool = False\n",
    ") -> list[TrainingExample]:\n",
    "    \"\"\"\n",
    "    Play one game of self-play and collect training data.\n",
    "    \n",
    "    Args:\n",
    "        network: Neural network for MCTS\n",
    "        num_simulations: MCTS simulations per move\n",
    "        temp_threshold: Move number after which temperature drops\n",
    "        verbose: Print game progress\n",
    "    \n",
    "    Returns:\n",
    "        List of training examples from the game\n",
    "    \"\"\"\n",
    "    mcts = AlphaMCTS(\n",
    "        network=network,\n",
    "        num_simulations=num_simulations,\n",
    "        add_noise=True  # Exploration noise during self-play\n",
    "    )\n",
    "    \n",
    "    game_state = GameState()\n",
    "    game_history = []  # (state, policy, current_player)\n",
    "    move_count = 0\n",
    "    \n",
    "    while not game_state.is_terminal():\n",
    "        # Temperature schedule: high early for exploration, low later\n",
    "        if move_count < temp_threshold:\n",
    "            temperature = TEMP_INIT\n",
    "        else:\n",
    "            temperature = TEMP_FINAL\n",
    "        \n",
    "        # Get MCTS policy\n",
    "        action_probs, _ = mcts.get_action_probs(game_state, temperature=temperature)\n",
    "        \n",
    "        # Store training data (before move)\n",
    "        encoded_state = encode_state(game_state)\n",
    "        current_player = game_state.current_player\n",
    "        game_history.append((encoded_state, action_probs, current_player))\n",
    "        \n",
    "        # Select and apply action\n",
    "        if temperature == 0:\n",
    "            action_idx = np.argmax(action_probs)\n",
    "        else:\n",
    "            action_idx = np.random.choice(len(action_probs), p=action_probs)\n",
    "        \n",
    "        action = index_to_action(action_idx)\n",
    "        game_state.apply_action(action)\n",
    "        \n",
    "        move_count += 1\n",
    "        \n",
    "        if verbose and move_count % 10 == 0:\n",
    "            print(f\"Move {move_count}, Player {game_state.current_player}'s turn\")\n",
    "    \n",
    "    # Game finished - assign values based on outcome\n",
    "    winner = game_state.winner\n",
    "    \n",
    "    training_examples = []\n",
    "    for state, policy, player in game_history:\n",
    "        # Value from this player's perspective\n",
    "        if winner is None:\n",
    "            value = 0.0  # Draw (shouldn't happen in Quoridor)\n",
    "        elif winner == player:\n",
    "            value = 1.0  # Win\n",
    "        else:\n",
    "            value = -1.0  # Loss\n",
    "        \n",
    "        training_examples.append(TrainingExample(\n",
    "            state=state,\n",
    "            policy=policy,\n",
    "            value=value\n",
    "        ))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Game finished in {move_count} moves. Winner: Player {winner}\")\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "\n",
    "def generate_self_play_data(\n",
    "    network: QuoridorNet,\n",
    "    num_games: int = SELF_PLAY_GAMES,\n",
    "    num_simulations: int = ALPHA_MCTS_SIMULATIONS,\n",
    "    verbose: bool = True\n",
    ") -> list[TrainingExample]:\n",
    "    \"\"\"\n",
    "    Generate training data through self-play.\n",
    "    \n",
    "    Args:\n",
    "        network: Neural network for MCTS\n",
    "        num_games: Number of self-play games\n",
    "        num_simulations: MCTS simulations per move\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        List of all training examples from all games\n",
    "    \"\"\"\n",
    "    all_examples = []\n",
    "    \n",
    "    for game_idx in range(num_games):\n",
    "        if verbose:\n",
    "            print(f\"\\nSelf-play game {game_idx + 1}/{num_games}\")\n",
    "        \n",
    "        examples = self_play_game(\n",
    "            network=network,\n",
    "            num_simulations=num_simulations,\n",
    "            verbose=False\n",
    "        )\n",
    "        all_examples.extend(examples)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Generated {len(examples)} examples (total: {len(all_examples)})\")\n",
    "    \n",
    "    return all_examples\n",
    "\n",
    "\n",
    "print(\"Self-play functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ffe7f8",
   "metadata": {},
   "source": [
    "## 4. Network Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(\n",
    "    network: QuoridorNet,\n",
    "    examples: list[TrainingExample],\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    learning_rate: float = LEARNING_RATE,\n",
    "    weight_decay: float = WEIGHT_DECAY,\n",
    "    epochs: int = 10,\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train the neural network on collected examples.\n",
    "    \n",
    "    Args:\n",
    "        network: Neural network to train\n",
    "        examples: Training examples\n",
    "        batch_size: Mini-batch size\n",
    "        learning_rate: Learning rate\n",
    "        weight_decay: L2 regularization\n",
    "        epochs: Training epochs over the data\n",
    "        verbose: Print training progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    device = next(network.parameters()).device\n",
    "    \n",
    "    # Create data loader\n",
    "    dataset = TrainingDataset(examples)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        network.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    policy_loss_fn = nn.CrossEntropyLoss()\n",
    "    value_loss_fn = nn.MSELoss()\n",
    "    \n",
    "    metrics = {\n",
    "        'policy_loss': [],\n",
    "        'value_loss': [],\n",
    "        'total_loss': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_policy_loss = 0.0\n",
    "        epoch_value_loss = 0.0\n",
    "        epoch_total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for states, policies, values in dataloader:\n",
    "            states = states.to(device)\n",
    "            policies = policies.to(device)\n",
    "            values = values.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            policy_logits, pred_values = network(states)\n",
    "            \n",
    "            # Policy loss (cross-entropy with MCTS policy as target)\n",
    "            policy_loss = -torch.mean(\n",
    "                torch.sum(policies * torch.log_softmax(policy_logits, dim=1), dim=1)\n",
    "            )\n",
    "            \n",
    "            # Value loss (MSE)\n",
    "            value_loss = value_loss_fn(pred_values, values)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = policy_loss + value_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_policy_loss += policy_loss.item()\n",
    "            epoch_value_loss += value_loss.item()\n",
    "            epoch_total_loss += total_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average losses for epoch\n",
    "        avg_policy = epoch_policy_loss / num_batches\n",
    "        avg_value = epoch_value_loss / num_batches\n",
    "        avg_total = epoch_total_loss / num_batches\n",
    "        \n",
    "        metrics['policy_loss'].append(avg_policy)\n",
    "        metrics['value_loss'].append(avg_value)\n",
    "        metrics['total_loss'].append(avg_total)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: \"\n",
    "                  f\"Policy={avg_policy:.4f}, Value={avg_value:.4f}, Total={avg_total:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583869b",
   "metadata": {},
   "source": [
    "## 5. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    num_iterations: int = TRAINING_ITERATIONS,\n",
    "    games_per_iteration: int = SELF_PLAY_GAMES,\n",
    "    simulations_per_move: int = ALPHA_MCTS_SIMULATIONS,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    resume_from: Optional[str] = None,\n",
    "    device: str = 'cpu'\n",
    ") -> QuoridorNet:\n",
    "    \"\"\"\n",
    "    Main AlphaZero training loop.\n",
    "    \n",
    "    Args:\n",
    "        num_iterations: Number of training iterations\n",
    "        games_per_iteration: Self-play games per iteration\n",
    "        simulations_per_move: MCTS simulations per move\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        resume_from: Path to checkpoint to resume from\n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    Returns:\n",
    "        Trained neural network\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize or load network\n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f\"Resuming from checkpoint: {resume_from}\")\n",
    "        network = create_network(device)\n",
    "        network.load_checkpoint(resume_from)\n",
    "    else:\n",
    "        print(\"Starting fresh training\")\n",
    "        network = create_network(device)\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "    \n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training Iteration {iteration}/{num_iterations}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Phase 1: Self-play\n",
    "        print(\"\\n--- Self-Play Phase ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        examples = generate_self_play_data(\n",
    "            network=network,\n",
    "            num_games=games_per_iteration,\n",
    "            num_simulations=simulations_per_move,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        replay_buffer.add_batch(examples)\n",
    "        \n",
    "        self_play_time = time.time() - start_time\n",
    "        print(f\"Self-play completed in {self_play_time:.1f}s\")\n",
    "        print(f\"Replay buffer size: {len(replay_buffer)}\")\n",
    "        \n",
    "        # Phase 2: Training\n",
    "        print(\"\\n--- Training Phase ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        training_examples = replay_buffer.sample(min(len(replay_buffer), BATCH_SIZE * 100))\n",
    "        \n",
    "        metrics = train_network(\n",
    "            network=network,\n",
    "            examples=training_examples,\n",
    "            epochs=10,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.1f}s\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if iteration % CHECKPOINT_INTERVAL == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_iter_{iteration}.pt\")\n",
    "            network.save_checkpoint(checkpoint_path)\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = os.path.join(checkpoint_dir, \"model_final.pt\")\n",
    "    network.save_checkpoint(final_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return network\n",
    "\n",
    "\n",
    "print(\"Training loop defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec82b04",
   "metadata": {},
   "source": [
    "## 6. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_against_random(\n",
    "    network: QuoridorNet,\n",
    "    num_games: int = 20,\n",
    "    simulations: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate trained network against random player.\n",
    "    \n",
    "    Args:\n",
    "        network: Trained neural network\n",
    "        num_games: Number of evaluation games\n",
    "        simulations: MCTS simulations per move\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with win rate statistics\n",
    "    \"\"\"\n",
    "    from AI.alpha_mcts import AlphaZeroPlayer\n",
    "    \n",
    "    wins = {'network': 0, 'random': 0, 'draws': 0}\n",
    "    \n",
    "    for game_idx in range(num_games):\n",
    "        # Alternate who goes first\n",
    "        network_is_p1 = (game_idx % 2 == 0)\n",
    "        \n",
    "        game_state = GameState()\n",
    "        \n",
    "        alpha_player = AlphaZeroPlayer(\n",
    "            player=1 if network_is_p1 else 2,\n",
    "            network=network,\n",
    "            num_simulations=simulations,\n",
    "            temperature=0.0  # Greedy play\n",
    "        )\n",
    "        \n",
    "        while not game_state.is_terminal():\n",
    "            current = game_state.current_player\n",
    "            is_network_turn = (current == 1) == network_is_p1\n",
    "            \n",
    "            if is_network_turn:\n",
    "                action = alpha_player.get_action(game_state)\n",
    "            else:\n",
    "                # Random player\n",
    "                actions = game_state.get_legal_actions()\n",
    "                action = random.choice(actions)\n",
    "            \n",
    "            game_state.apply_action(action)\n",
    "        \n",
    "        winner = game_state.winner\n",
    "        if winner is None:\n",
    "            wins['draws'] += 1\n",
    "        elif (winner == 1) == network_is_p1:\n",
    "            wins['network'] += 1\n",
    "        else:\n",
    "            wins['random'] += 1\n",
    "    \n",
    "    total = num_games\n",
    "    results = {\n",
    "        'network_wins': wins['network'],\n",
    "        'random_wins': wins['random'],\n",
    "        'draws': wins['draws'],\n",
    "        'network_win_rate': wins['network'] / total,\n",
    "        'games_played': total\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluation Results ({num_games} games):\")\n",
    "    print(f\"  Network wins: {wins['network']} ({100*wins['network']/total:.1f}%)\")\n",
    "    print(f\"  Random wins:  {wins['random']} ({100*wins['random']/total:.1f}%)\")\n",
    "    print(f\"  Draws:        {wins['draws']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec10092",
   "metadata": {},
   "source": [
    "## 7. Run Training\n",
    "\n",
    "Configure and run the training pipeline. Adjust parameters as needed for your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d464b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_ITERATIONS = 5          # Number of training iterations\n",
    "GAMES_PER_ITERATION = 10    # Self-play games per iteration\n",
    "SIMULATIONS_PER_MOVE = 50   # MCTS simulations per move\n",
    "CHECKPOINT_DIR = \"../checkpoints\"  # Relative to AI folder\n",
    "\n",
    "# Device selection\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a45306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting AlphaZero training for Quoridor...\")\n",
    "print(\"Note: This is computationally intensive!\\n\")\n",
    "\n",
    "network = training_loop(\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    games_per_iteration=GAMES_PER_ITERATION,\n",
    "    simulations_per_move=SIMULATIONS_PER_MOVE,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73fd09",
   "metadata": {},
   "source": [
    "## 8. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cba735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against random player\n",
    "print(\"=\"*50)\n",
    "print(\"Evaluating trained model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = evaluate_against_random(\n",
    "    network=network,\n",
    "    num_games=10,\n",
    "    simulations=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2dd53",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Metrics (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to visualize training progress, uncomment and run:\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# # Plot training metrics from the last training run\n",
    "# # You would need to save metrics during training to plot them\n",
    "# print(\"Training visualization would go here\")\n",
    "# print(\"Consider saving metrics during training_loop for visualization\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
